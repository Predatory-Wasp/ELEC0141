{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "# é­”æ³•å‘½ä»¤ï¼Œä½¿ç”¨åç”»å›¾ä¸ç”¨showäº†\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re# å¼•å…¥æ­£åˆ™\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.è§£å‹è¯å‘é‡å¹¶åŠ è½½"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1è§£å‹è¯å‘é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2# ç”¨æ¥è§£å‹æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./embeddings/sgns.weibo.bigram\", 'wb') as new_file, open(\"./embeddings/sgns.weibo.bigram.bz2\", 'rb') as file:\n",
    "    decompressor = bz2.BZ2Decompressor()\n",
    "    for data in iter(lambda : file.read(100 * 1024), b''):\n",
    "        new_file.write(decompressor.decompress(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2åŠ è½½è¯å‘é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors# gensimç”¨æ¥åŠ è½½é¢„è®­ç»ƒè¯å‘é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_model = KeyedVectors.load_word2vec_format('./embeddings/sgns.weibo.bigram', \n",
    "                                             binary=False,\n",
    "                                             unicode_errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.è¯­æ–™é¢„å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1è¯»å–åŸå§‹æ–‡æœ¬\n",
    "* weiboï¼šDataFrameå­˜å‚¨çš„åšæ–‡åŠå…¶å¯¹åº”æ ‡ç­¾\n",
    "* contentï¼šlistå­˜å‚¨çš„åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²\n",
    "* labelï¼šæ ‡ç­¾ï¼Œ1ä¸ºéè°£è¨€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_not_rumor</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ã€è¶…å¸‚æ‰‹æ¨è½¦æ’å…¥å¼æ‘†æ”¾å®›è‹¥åŒæ€§çˆ±å§¿åŠ¿ï¼Œä¸“å®¶å»ºè®®å–ç¼”[æ±—]ã€‘æ•™è‚²ä¸“å®¶ç‹å»ºç«‹è¿‘æ—¥æŒ‡å‡ºï¼Œè¶…å¸‚çš„æ‰‹æ¨...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ç´§æ€¥é€šçŸ¥ï¼Œæ±•å¤´å¸‚å‡ºäº‹äº†ï¼æ­é˜³å¸‚å‡ºäº‹äº†ï¼æ½®å·ã€æ±•å°¾ã€æ·±åœ³ã€å¹¿å·éƒ½ç›¸ç»§å‡ºäº‹äº†ï¼å¤§å®¶ä¸€å®šè¦äº’ç›¸è½¬å‘Š...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>åˆ†æ‰‹åï¼Œä¸è¦å›æƒ³ç”œèœœå¾€äº‹ï¼Œå› ä¸ºä¼šè®©è‡ªå·±æ›´ç—›è‹¦ï¼›ä¸è¦æ€€ç–‘TAçš„å†³å®šï¼Œå› ä¸ºTAå·²ç»å†³å®šäº†ï¼›ä¸è¦å°...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>æ³¨æ„å•¦ï¼ã€æ‰“é’ˆè¥¿ç“œã€‘å…¥å¤ï¼Œè¥¿ç“œæˆä¸ºé¦–é€‰çš„æ¶ˆæš‘é£Ÿå“ï¼Œä½†é»‘å¿ƒå•†è´©å´æŠŠé’ˆå¤´å¯¹å‡†äº†å°šæœªæˆç†Ÿçš„è¥¿ç“œã€‚â€œ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2012å¹´2æœˆ3æ—¥å½“åœ°æ—¶é—´ä¸‹åˆ3ç‚¹41åˆ†æœ‰ä¸ªå°åº¦å¦‡å¥³ç”Ÿè‚²äº†11ä¸ªå°å­©ï¼ ç¬é—´å°±è¢«éœ‡æƒŠäº†</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_not_rumor                                            content\n",
       "0             0  ã€è¶…å¸‚æ‰‹æ¨è½¦æ’å…¥å¼æ‘†æ”¾å®›è‹¥åŒæ€§çˆ±å§¿åŠ¿ï¼Œä¸“å®¶å»ºè®®å–ç¼”[æ±—]ã€‘æ•™è‚²ä¸“å®¶ç‹å»ºç«‹è¿‘æ—¥æŒ‡å‡ºï¼Œè¶…å¸‚çš„æ‰‹æ¨...\n",
       "1             0  ç´§æ€¥é€šçŸ¥ï¼Œæ±•å¤´å¸‚å‡ºäº‹äº†ï¼æ­é˜³å¸‚å‡ºäº‹äº†ï¼æ½®å·ã€æ±•å°¾ã€æ·±åœ³ã€å¹¿å·éƒ½ç›¸ç»§å‡ºäº‹äº†ï¼å¤§å®¶ä¸€å®šè¦äº’ç›¸è½¬å‘Š...\n",
       "2             1  åˆ†æ‰‹åï¼Œä¸è¦å›æƒ³ç”œèœœå¾€äº‹ï¼Œå› ä¸ºä¼šè®©è‡ªå·±æ›´ç—›è‹¦ï¼›ä¸è¦æ€€ç–‘TAçš„å†³å®šï¼Œå› ä¸ºTAå·²ç»å†³å®šäº†ï¼›ä¸è¦å°...\n",
       "3             0  æ³¨æ„å•¦ï¼ã€æ‰“é’ˆè¥¿ç“œã€‘å…¥å¤ï¼Œè¥¿ç“œæˆä¸ºé¦–é€‰çš„æ¶ˆæš‘é£Ÿå“ï¼Œä½†é»‘å¿ƒå•†è´©å´æŠŠé’ˆå¤´å¯¹å‡†äº†å°šæœªæˆç†Ÿçš„è¥¿ç“œã€‚â€œ...\n",
       "4             0        2012å¹´2æœˆ3æ—¥å½“åœ°æ—¶é—´ä¸‹åˆ3ç‚¹41åˆ†æœ‰ä¸ªå°åº¦å¦‡å¥³ç”Ÿè‚²äº†11ä¸ªå°å­©ï¼ ç¬é—´å°±è¢«éœ‡æƒŠäº†"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weibo = pd.read_csv('./data/all_data.txt',sep='\\t', names=['is_not_rumor','content'],encoding='utf-8')\n",
    "weibo = weibo.dropna()#åˆ é™¤ç¼ºå¤±å€¼\n",
    "weibo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3387, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weibo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å°†DataFrameä¸­çš„Seriesè½¬æ¢ä¸ºlist\n",
    "content = weibo.content.values.tolist()\n",
    "label=weibo.is_not_rumor.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['æ³¨æ„å•¦ï¼ã€æ‰“é’ˆè¥¿ç“œã€‘å…¥å¤ï¼Œè¥¿ç“œæˆä¸ºé¦–é€‰çš„æ¶ˆæš‘é£Ÿå“ï¼Œä½†é»‘å¿ƒå•†è´©å´æŠŠé’ˆå¤´å¯¹å‡†äº†å°šæœªæˆç†Ÿçš„è¥¿ç“œã€‚â€œæ‰“é’ˆè¥¿ç“œâ€æ‰€æ³¨å°„çš„ç¦ç”¨é£Ÿå“æ·»åŠ å‰‚ç”œèœœç´ å’Œèƒ­è„‚çº¢ï¼æ‰“è¿‡é’ˆçš„è¥¿ç“œç“œç“¤å‘ˆçº¢è‰²ï¼Œæ±æ¶²ä¹Ÿå¾ˆâ€œä¸°å¯Œâ€ï¼Œä½†æ²¡æœ‰ä¸€ç‚¹è¥¿ç“œå‘³ã€‚æ‰€ç”¨æ·»åŠ å‰‚ç ´å è‚è„ã€è‚¾è„çš„åŠŸèƒ½ã€å½±å“å„¿ç«¥æ™ºåŠ›å‘è‚²ç­‰æ¯’æ€§ï¼', '2012å¹´2æœˆ3æ—¥å½“åœ°æ—¶é—´ä¸‹åˆ3ç‚¹41åˆ†æœ‰ä¸ªå°åº¦å¦‡å¥³ç”Ÿè‚²äº†11ä¸ªå°å­©ï¼ ç¬é—´å°±è¢«éœ‡æƒŠäº†']\n"
     ]
    }
   ],
   "source": [
    "print (content[3:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2è¿›è¡Œåˆ†è¯å’Œtokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/lancopku/PKUSeg-python\n",
    "\n",
    "å¯¹æ¯ä¸€æ¡å¾®åšæ–‡æœ¬textï¼Œ\n",
    "1. å»æ‰æ¯ä¸ªæ ·æœ¬çš„æ ‡ç‚¹ç¬¦å·ï¼›\n",
    "2. ç”¨pkusegåˆ†è¯ï¼Œå¾—åˆ°å­˜æ”¾åˆ†è¯ç»“æœçš„cut_listï¼›\n",
    "3. å»æ‰cut_listä¸­çš„åœç”¨è¯å¾—åˆ°cut_list_cleanï¼›\n",
    "3. å°†åˆ†è¯ç»“æœcut_list_cleanç´¢å¼•åŒ–ï¼ˆä½¿ç”¨åŒ—äº¬å¸ˆèŒƒå¤§å­¦ä¸­æ–‡ä¿¡æ¯å¤„ç†ç ”ç©¶æ‰€ä¸ä¸­å›½äººæ°‘å¤§å­¦ DBIIR å®éªŒå®¤çš„ç ”ç©¶è€…å¼€æºçš„\"chinese-word-vectors\"ï¼‰ï¼Œè¿™æ ·æ¯ä¸€ä¾‹è¯„ä»·çš„æ–‡æœ¬å˜æˆä¸€æ®µç´¢å¼•æ•°å­—ï¼Œå¯¹åº”ç€é¢„è®­ç»ƒè¯å‘é‡æ¨¡å‹ä¸­çš„è¯ã€‚\n",
    "\n",
    "å°†æ¯ä¸ªtextçš„ç»“æœå­˜åˆ°train_tokensä¸­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkuseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å¯¼å…¥åœç”¨è¯\n",
    "stopwords=pd.read_csv(\"./stopwords/stopwords.txt\",index_col=False,sep=\"\\t\",quoting=3,names=['stopword'], encoding='utf-8')\n",
    "stopwords = stopwords.stopword.values.tolist()#è½¬ä¸ºlistå½¢å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = pkuseg.pkuseg(model_name='web')  # ç¨‹åºä¼šè‡ªåŠ¨ä¸‹è½½æ‰€å¯¹åº”çš„ç»†é¢†åŸŸæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = []\n",
    "for text in content:\n",
    "    # å»æ‰æ ‡ç‚¹\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+â€”â€”ï¼ï¼Œã€‚ï¼Ÿã€~@#ï¿¥%â€¦â€¦&*ï¼ˆï¼‰]+\", \"\",text)\n",
    "    # pkusegåˆ†è¯\n",
    "    cut_list = seg.cut(text)\n",
    "\n",
    "    #å»é™¤åœç”¨è¯\n",
    "    cut_list_clean=[]\n",
    "    for word in cut_list:\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        cut_list_clean.append(word)\n",
    "    \n",
    "    #ç´¢å¼•åŒ–\n",
    "    for i, word in enumerate(cut_list_clean): # enumerate()\n",
    "        try:\n",
    "            # å°†è¯è½¬æ¢ä¸ºç´¢å¼•index\n",
    "            cut_list_clean[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            # å¦‚æœè¯ä¸åœ¨å­—å…¸ä¸­ï¼Œåˆ™è¾“å‡º0\n",
    "            cut_list_clean[i] = 0\n",
    "    train_tokens.append(cut_list_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3ç´¢å¼•é•¿åº¦æ ‡å‡†åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å› ä¸ºæ¯æ®µè¯„è¯­çš„é•¿åº¦æ˜¯ä¸ä¸€æ ·çš„ï¼Œå¦‚æœå•çº¯å–æœ€é•¿çš„ä¸€ä¸ªè¯„è¯­ï¼Œå¹¶æŠŠå…¶ä»–è¯„å¡«å……æˆåŒæ ·çš„é•¿åº¦ï¼Œè¿™æ ·ååˆ†æµªè´¹è®¡ç®—èµ„æºï¼Œæ‰€ä»¥å–ä¸€ä¸ªæŠ˜è¡·çš„é•¿åº¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è·å¾—æ‰€æœ‰tokensçš„é•¿åº¦\n",
    "num_tokens = [len(tokens) for tokens in train_tokens]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# å–tokenså¹³å‡å€¼å¹¶åŠ ä¸Šä¸¤ä¸ªtokensçš„æ ‡å‡†å·®ï¼Œ\n",
    "# å‡è®¾tokensé•¿åº¦çš„åˆ†å¸ƒä¸ºæ­£æ€åˆ†å¸ƒï¼Œåˆ™max_tokensè¿™ä¸ªå€¼å¯ä»¥æ¶µç›–95%å·¦å³çš„æ ·æœ¬\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### paddingï¼ˆå¡«å……ï¼‰å’Œtruncatingï¼ˆä¿®å‰ªï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬æŠŠæ–‡æœ¬è½¬æ¢ä¸ºtokensï¼ˆç´¢å¼•ï¼‰ä¹‹åï¼Œæ¯ä¸€ä¸²ç´¢å¼•çš„é•¿åº¦å¹¶ä¸ç›¸ç­‰ï¼Œæ‰€ä»¥ä¸ºäº†æ–¹ä¾¿æ¨¡å‹çš„è®­ç»ƒæˆ‘ä»¬éœ€è¦æŠŠç´¢å¼•çš„é•¿åº¦æ ‡å‡†åŒ–ï¼Œä¸Šé¢æˆ‘ä»¬é€‰æ‹©äº†max_tokensä¸ªå¯ä»¥æ¶µç›–95%è®­ç»ƒæ ·æœ¬çš„é•¿åº¦ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬è¿›è¡Œpaddingå’Œtruncatingï¼Œæˆ‘ä»¬ä¸€èˆ¬é‡‡ç”¨'pre'çš„æ–¹æ³•ï¼Œè¿™ä¼šåœ¨æ–‡æœ¬ç´¢å¼•çš„å‰é¢å¡«å……0ï¼Œå› ä¸ºæ ¹æ®ä¸€äº›ç ”ç©¶èµ„æ–™ä¸­çš„å®è·µï¼Œå¦‚æœåœ¨æ–‡æœ¬ç´¢å¼•åé¢å¡«å……0çš„è¯ï¼Œä¼šå¯¹æ¨¡å‹é€ æˆä¸€äº›ä¸è‰¯å½±å“ã€‚ \n",
    "\n",
    "è¿›è¡Œpaddingå’Œtruncatingï¼Œ è¾“å…¥çš„train_tokensæ˜¯ä¸€ä¸ªlist\n",
    "è¿”å›çš„train_padæ˜¯ä¸€ä¸ªnumpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pad = pad_sequences(train_tokens, maxlen=max_tokens,\n",
    "                            padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4å‡†å¤‡Embedding Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ä½œä¸ºæ¨¡å‹çš„è¾“å…¥ï¼Œéœ€è¦å‡†å¤‡ä¸€ä¸ªç»´åº¦ä¸º (ğ‘›ğ‘¢ğ‘šğ‘¤ğ‘œğ‘Ÿğ‘‘ğ‘ ,ğ‘’ğ‘šğ‘ğ‘’ğ‘‘ğ‘‘ğ‘–ğ‘›ğ‘”ğ‘‘ğ‘–ğ‘š) çš„embeddingçŸ©é˜µï¼Œnum wordsä»£è¡¨ä½¿ç”¨çš„è¯æ±‡çš„æ•°é‡ã€‚\n",
    "\n",
    "\n",
    "* ä¸è¿›è¡Œè¯å‘é‡çš„è®­ç»ƒï¼Œè€Œæ˜¯ä½¿ç”¨é¢„è®­ç»ƒçš„è¯å‘é‡â€”â€”åŒ—äº¬å¸ˆèŒƒå¤§å­¦ä¸­æ–‡ä¿¡æ¯å¤„ç†ç ”ç©¶æ‰€ä¸ä¸­å›½äººæ°‘å¤§å­¦ DBIIR å®éªŒå®¤çš„ç ”ç©¶è€…å¼€æºçš„\"chinese-word-vectors\"ï¼›https://github.com/Embedding/Chinese-Word-Vectors ï¼›emdedding dimensionåœ¨ç°åœ¨ä½¿ç”¨çš„é¢„è®­ç»ƒè¯å‘é‡æ¨¡å‹ä¸­æ˜¯300ï¼Œæ¯ä¸€ä¸ªè¯æ±‡éƒ½ç”¨ä¸€ä¸ªé•¿åº¦ä¸º300çš„å‘é‡è¡¨ç¤ºã€‚\n",
    "\n",
    "\n",
    "* æ³¨æ„åªé€‰æ‹©ä½¿ç”¨å‰50kä¸ªä½¿ç”¨é¢‘ç‡æœ€é«˜çš„è¯ï¼Œåœ¨è¿™ä¸ªé¢„è®­ç»ƒè¯å‘é‡æ¨¡å‹ä¸­ï¼Œä¸€å…±æœ‰260ä¸‡è¯æ±‡é‡ï¼Œå¦‚æœå…¨éƒ¨ä½¿ç”¨åœ¨åˆ†ç±»é—®é¢˜ä¸Šä¼šå¾ˆæµªè´¹è®¡ç®—èµ„æºï¼Œå› ä¸ºè®­ç»ƒæ ·æœ¬å¾ˆå°ï¼Œå¦‚æœæœ‰æ›´å¤šçš„è®­ç»ƒæ ·æœ¬æ—¶ï¼Œåœ¨åˆ†ç±»é—®é¢˜ä¸Šå¯ä»¥è€ƒè™‘å‡å°‘ä½¿ç”¨çš„è¯æ±‡é‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words = 50000\n",
    "embedding_dim=300\n",
    "# åˆå§‹åŒ–embedding_matrixï¼Œä¹‹ååœ¨kerasä¸Šè¿›è¡Œåº”ç”¨\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "# embedding_matrixä¸ºä¸€ä¸ª [num_wordsï¼Œembedding_dim] çš„çŸ©é˜µ\n",
    "# ç»´åº¦ä¸º 50000 * 300\n",
    "for i in range(num_words):\n",
    "    embedding_matrix[i,:] = cn_model[cn_model.index2word[i]]#å‰50000ä¸ªindexå¯¹åº”çš„è¯çš„è¯å‘é‡\n",
    "embedding_matrix = embedding_matrix.astype('float32')\n",
    "# æ£€æŸ¥indexæ˜¯å¦å¯¹åº”ï¼Œ\n",
    "# è¾“å‡º300æ„ä¹‰ä¸ºé•¿åº¦ä¸º300çš„embeddingå‘é‡ä¸€ä¸€å¯¹åº”\n",
    "np.sum(cn_model[cn_model.index2word[333]] == embedding_matrix[333] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¶…å‡ºäº”ä¸‡ä¸ªè¯å‘é‡çš„è¯ç”¨0ä»£æ›¿\n",
    "train_pad[train_pad>=num_words ] = 0\n",
    "\n",
    "# å‡†å¤‡targetå‘é‡ï¼Œå‰2000æ ·æœ¬ä¸º1ï¼Œå2000ä¸º0\n",
    "train_target = np.array(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.è®­ç»ƒè¯­æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•æ ·æœ¬çš„åˆ†å‰²\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 90%çš„æ ·æœ¬ç”¨æ¥è®­ç»ƒï¼Œå‰©ä½™10%ç”¨æ¥æµ‹è¯•\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pad,\n",
    "                                                    train_target,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2æ­å»ºç½‘ç»œç»“æ„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ç”¨kerasæ­å»ºLSTMæ¨¡å‹ï¼Œæ¨¡å‹çš„ç¬¬ä¸€å±‚æ˜¯Embeddingå±‚ï¼Œåªæœ‰å½“æˆ‘ä»¬æŠŠtokensç´¢å¼•è½¬æ¢ä¸ºè¯å‘é‡çŸ©é˜µä¹‹åï¼Œæ‰å¯ä»¥ç”¨ç¥ç»ç½‘ç»œå¯¹æ–‡æœ¬è¿›è¡Œå¤„ç†ã€‚ kerasæä¾›äº†Embeddingæ¥å£ï¼Œé¿å…äº†ç¹ççš„ç¨€ç–çŸ©é˜µæ“ä½œã€‚\n",
    "* åœ¨Embeddingå±‚æˆ‘ä»¬è¾“å…¥çš„çŸ©é˜µä¸ºï¼š(ğ‘ğ‘ğ‘¡ğ‘â„ğ‘ ğ‘–ğ‘§ğ‘’,ğ‘šğ‘ğ‘¥ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘ )è¾“å‡ºçŸ©é˜µä¸º:(ğ‘ğ‘ğ‘¡ğ‘â„ğ‘ ğ‘–ğ‘§ğ‘’,ğ‘šğ‘ğ‘¥ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘ ,ğ‘’ğ‘šğ‘ğ‘’ğ‘‘ğ‘‘ğ‘–ğ‘›ğ‘”ğ‘‘ğ‘–ğ‘š)ã€‚\n",
    "* ä½¿ç”¨é¢„è®­ç»ƒçš„è¯å‘é‡ï¼Œå°†trainableè®¾ä¸ºFalseï¼Œå³ä¸å¯è®­ç»ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_tokens,\n",
    "                    trainable=False))\n",
    "model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(units=32, return_sequences=False)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "optimizer=Adam(lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================model1=================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_tokens,\n",
    "                    trainable=False))\n",
    "model1.add(Bidirectional(GRU(32)))\n",
    "model1.add(Dense(6, activation='relu'))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "optimizer=Adam(lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3æ¨¡å‹é…ç½®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ¨¡å‹ä¿å­˜ï¼ˆæ–­ç‚¹ç»­è®­ï¼‰ã€early stopingã€å­¦ä¹ ç‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹ä¸€ä¸ªæƒé‡çš„å­˜å‚¨ç‚¹\n",
    "checkpoint_save_path=\"./checkpoint/rumor_LSTM.ckpt\"\n",
    "if os.path.exists(checkpoint_save_path+'.index'):\n",
    "    print('----------load the model----------')\n",
    "    model.load_weights(checkpoint_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ä¿å­˜å‚æ•°å’Œæ¨¡å‹\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_save_path, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰early stopingå¦‚æœ3ä¸ªepochå†…validation lossæ²¡æœ‰æ”¹å–„åˆ™åœæ­¢è®­ç»ƒ\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "# è‡ªåŠ¨é™ä½learning rate\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1, min_lr=1e-8, patience=0,\n",
    "                                       verbose=1)\n",
    "# å®šä¹‰callbackå‡½æ•°\n",
    "callbacks = [\n",
    "    earlystopping, \n",
    "#    checkpoint,\n",
    "    lr_reduction\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================model1==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹ä¸€ä¸ªæƒé‡çš„å­˜å‚¨ç‚¹\n",
    "checkpoint_save_path1=\"./checkpoint/rumor_GRU.ckpt\"\n",
    "if os.path.exists(checkpoint_save_path1+'.index'):\n",
    "    print('----------load the model----------')\n",
    "    model1.load_weights(checkpoint_save_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ä¿å­˜å‚æ•°å’Œæ¨¡å‹\n",
    "checkpoint1 = ModelCheckpoint(filepath=checkpoint_save_path1, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4è®­ç»ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "22/22 [==============================] - 14s 214ms/step - loss: 0.6209 - accuracy: 0.6591 - val_loss: 0.5098 - val_accuracy: 0.7541 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 3s 128ms/step - loss: 0.4550 - accuracy: 0.7871 - val_loss: 0.4417 - val_accuracy: 0.8033 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 3s 117ms/step - loss: 0.3961 - accuracy: 0.8236 - val_loss: 0.4272 - val_accuracy: 0.8066 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 3s 126ms/step - loss: 0.3300 - accuracy: 0.8611 - val_loss: 0.3843 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.2766 - accuracy: 0.8822\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "22/22 [==============================] - 3s 127ms/step - loss: 0.2766 - accuracy: 0.8822 - val_loss: 0.3914 - val_accuracy: 0.8295 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "22/22 [==============================] - 3s 123ms/step - loss: 0.2105 - accuracy: 0.9202 - val_loss: 0.3605 - val_accuracy: 0.8590 - lr: 1.0000e-04\n",
      "Epoch 7/20\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.2033 - accuracy: 0.9231\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "22/22 [==============================] - 3s 122ms/step - loss: 0.2033 - accuracy: 0.9231 - val_loss: 0.3628 - val_accuracy: 0.8557 - lr: 1.0000e-04\n",
      "Epoch 8/20\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.1939 - accuracy: 0.9293\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "22/22 [==============================] - 3s 127ms/step - loss: 0.1939 - accuracy: 0.9293 - val_loss: 0.3631 - val_accuracy: 0.8590 - lr: 1.0000e-05\n",
      "Epoch 9/20\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.1930 - accuracy: 0.9289\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "22/22 [==============================] - 3s 127ms/step - loss: 0.1930 - accuracy: 0.9289 - val_loss: 0.3632 - val_accuracy: 0.8590 - lr: 1.0000e-06\n",
      "Epoch 10/20\n",
      "18/22 [=======================>......] - ETA: 0s - loss: 0.1940 - accuracy: 0.9297"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train,validation_split=0.1,epochs=20,batch_size=128,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================model1==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit(X_train, y_train,validation_split=0.1,epochs=20,batch_size=128,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5åº”ç”¨äºæµ‹è¯•é›†\n",
    "é¦–å…ˆå¯¹æµ‹è¯•æ ·æœ¬è¿›è¡Œé¢„æµ‹ï¼Œå¾—åˆ°äº†è¿˜ç®—æ»¡æ„çš„å‡†ç¡®åº¦ã€‚ä¹‹åæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªé¢„æµ‹å‡½æ•°ï¼Œæ¥é¢„æµ‹è¾“å…¥çš„æ–‡æœ¬çš„ææ€§ï¼Œå¯è§æ¨¡å‹å¯¹äºå¦å®šå¥å’Œä¸€äº›ç®€å•çš„é€»è¾‘ç»“æ„éƒ½å¯ä»¥è¿›è¡Œå‡†ç¡®çš„åˆ¤æ–­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================model1==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model1.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6å®ä¾‹å±•ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rumor_LSTM(text,label):\n",
    "    print(text)\n",
    "    # å»æ ‡ç‚¹\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+â€”â€”ï¼ï¼Œã€‚ï¼Ÿã€~@#ï¿¥%â€¦â€¦&*ï¼ˆï¼‰]+\", \"\",text)\n",
    "    # åˆ†è¯\n",
    "    cut = seg.cut(text)\n",
    "    #å»é™¤åœç”¨è¯\n",
    "    cut_clean=[]\n",
    "    for word in cut:\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        cut_clean.append(word)\n",
    "    # tokenize\n",
    "    for i, word in enumerate(cut_clean):\n",
    "        try:\n",
    "            cut_clean[i] = cn_model.vocab[word].index\n",
    "            if cut_clean[i] >= 50000:\n",
    "                cut_clean[i] = 0\n",
    "        except KeyError:\n",
    "            cut_clean[i] = 0\n",
    "    # padding\n",
    "    tokens_pad = pad_sequences([cut_clean], maxlen=max_tokens,\n",
    "                           padding='pre', truncating='pre')\n",
    "    # é¢„æµ‹\n",
    "    dic={0:'è°£è¨€',1:'éè°£è¨€'}\n",
    "    result = model.predict(x=tokens_pad)\n",
    "    coef = result[0][0]\n",
    "    if coef >= 0.5:\n",
    "        print('çœŸå®æ˜¯'+dic[label],'é¢„æµ‹æ˜¯éè°£è¨€','output=%.2f'%coef)\n",
    "    else:\n",
    "        print('çœŸå®æ˜¯'+dic[label],'é¢„æµ‹æ˜¯è°£è¨€','output=%.2f'%coef)\n",
    "    print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = [\n",
    "    'å…´ä»å¿ä»Šå¤©æŠ¢å°å­©æ²¡æŠ¢èµ°ï¼ŒæŠŠå­©å­æ¯äº²æ…äº†ä¸€åˆ€ï¼Œçœ‹è§è¿™è½¦çš„æ³¨æ„äº†ï¼ŒçœŸäº‹ï¼Œè½¦ç‰Œå·è¾½HFM055ï¼ï¼ï¼ï¼ï¼èµ¶ç´§æ•£æ’­ï¼ éƒ½åˆ«å¸¦å­©å­å‡ºå»çè½¬æ‚ äº† å°¤å…¶åˆ«è®©è€äººè‡ªå·±å¸¦å­©å­å‡ºå» å¤ªå±é™©äº† æ³¨æ„äº†ï¼ï¼ï¼ï¼è¾½HFM055åŒ—äº¬ç°ä»£æœ—åŠ¨ï¼Œåœ¨å„å­¦æ ¡é—¨å£æŠ¢å°å­©ï¼ï¼ï¼110å·²ç» è¯å®ï¼ï¼å…¨å¸‚é€šç¼‰ï¼ï¼',\n",
    "    'é‡åº†çœŸå®æ–°é—»:2016å¹´6æœˆ1æ—¥åœ¨é‡åº†æ¢å¹³å¿è¢é©¿é•‡å‘ç”Ÿä¸€èµ·æŠ¢å„¿ç«¥äº‹ä»¶ï¼Œåšæ¡ˆäººä¸‰ä¸ªä¸­å¹´ç”·äººï¼Œåœ¨ä¸‰ä¸­å­¦æ ¡åˆ°é•‡è¡—ä¸Šçš„ä¸€æ¡å°è·¯ä¸Šï¼ŒæŠŠå°å­©ç›´æ¥å¼„æ™•(å„¿ç«¥æ˜¯è¢é©¿æ–°å¹¼å„¿å›­ä¸­ç­çš„ä¸€åå­¦ç”Ÿ)ï¼Œæ­£å‡†å¤‡å¸¦èµ°æ—¶è¢«å®¶é•¿åŠæ—¶å‘ç°ç”¨æ£’å­èµ¶èµ°äº†åšæ¡ˆäººï¼Œæ•…æ­¤è·æ•‘ï¼è¯·å„ä½åŒèƒä»¬ä»¥æ­¤å¼•èµ·éå¸¸é‡è§†ï¼Œå¸Œæœ›å¤§å®¶æœ‰çˆ±å¿ƒçš„äººä¼ é€’ä¸‹',\n",
    "    '@å°¾ç†ŠC è¦æå‰é¢„ä¹ è‚²å„¿çŸ¥è¯†çš„è¯ï¼Œå»ºè®®çœ‹ä¸€äº›å°å·«å†™çš„ä¹¦ï¼Œå˜»å˜»',\n",
    "]\n",
    "test_label=[0,0,1]\n",
    "for i in range(len(test_list)):\n",
    "    predict_rumor_LSTM(test_list[i],test_label[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=====================model1========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rumor_GRU(text,label):\n",
    "    print(text)\n",
    "    # å»æ ‡ç‚¹\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+â€”â€”ï¼ï¼Œã€‚ï¼Ÿã€~@#ï¿¥%â€¦â€¦&*ï¼ˆï¼‰]+\", \"\",text)\n",
    "    # åˆ†è¯\n",
    "    cut = seg.cut(text)\n",
    "    #å»é™¤åœç”¨è¯\n",
    "    cut_clean=[]\n",
    "    for word in cut:\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        cut_clean.append(word)\n",
    "    # tokenize\n",
    "    for i, word in enumerate(cut_clean):\n",
    "        try:\n",
    "            cut_clean[i] = cn_model.vocab[word].index\n",
    "            if cut_clean[i] >= 50000:\n",
    "                cut_clean[i] = 0\n",
    "        except KeyError:\n",
    "            cut_clean[i] = 0\n",
    "    # padding\n",
    "    tokens_pad = pad_sequences([cut_clean], maxlen=max_tokens,\n",
    "                           padding='pre', truncating='pre')\n",
    "    # é¢„æµ‹\n",
    "    dic={0:'è°£è¨€',1:'éè°£è¨€'}\n",
    "    result = model1.predict(x=tokens_pad)\n",
    "    coef = result[0][0]\n",
    "    if coef >= 0.5:\n",
    "        print('çœŸå®æ˜¯'+dic[label],'é¢„æµ‹æ˜¯éè°£è¨€','output=%.2f'%coef)\n",
    "    else:\n",
    "        print('çœŸå®æ˜¯'+dic[label],'é¢„æµ‹æ˜¯è°£è¨€','output=%.2f'%coef)\n",
    "    print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = [\n",
    "    'å…´ä»å¿ä»Šå¤©æŠ¢å°å­©æ²¡æŠ¢èµ°ï¼ŒæŠŠå­©å­æ¯äº²æ…äº†ä¸€åˆ€ï¼Œçœ‹è§è¿™è½¦çš„æ³¨æ„äº†ï¼ŒçœŸäº‹ï¼Œè½¦ç‰Œå·è¾½HFM055ï¼ï¼ï¼ï¼ï¼èµ¶ç´§æ•£æ’­ï¼ éƒ½åˆ«å¸¦å­©å­å‡ºå»çè½¬æ‚ äº† å°¤å…¶åˆ«è®©è€äººè‡ªå·±å¸¦å­©å­å‡ºå» å¤ªå±é™©äº† æ³¨æ„äº†ï¼ï¼ï¼ï¼è¾½HFM055åŒ—äº¬ç°ä»£æœ—åŠ¨ï¼Œåœ¨å„å­¦æ ¡é—¨å£æŠ¢å°å­©ï¼ï¼ï¼110å·²ç» è¯å®ï¼ï¼å…¨å¸‚é€šç¼‰ï¼ï¼',\n",
    "    'é‡åº†çœŸå®æ–°é—»:2016å¹´6æœˆ1æ—¥åœ¨é‡åº†æ¢å¹³å¿è¢é©¿é•‡å‘ç”Ÿä¸€èµ·æŠ¢å„¿ç«¥äº‹ä»¶ï¼Œåšæ¡ˆäººä¸‰ä¸ªä¸­å¹´ç”·äººï¼Œåœ¨ä¸‰ä¸­å­¦æ ¡åˆ°é•‡è¡—ä¸Šçš„ä¸€æ¡å°è·¯ä¸Šï¼ŒæŠŠå°å­©ç›´æ¥å¼„æ™•(å„¿ç«¥æ˜¯è¢é©¿æ–°å¹¼å„¿å›­ä¸­ç­çš„ä¸€åå­¦ç”Ÿ)ï¼Œæ­£å‡†å¤‡å¸¦èµ°æ—¶è¢«å®¶é•¿åŠæ—¶å‘ç°ç”¨æ£’å­èµ¶èµ°äº†åšæ¡ˆäººï¼Œæ•…æ­¤è·æ•‘ï¼è¯·å„ä½åŒèƒä»¬ä»¥æ­¤å¼•èµ·éå¸¸é‡è§†ï¼Œå¸Œæœ›å¤§å®¶æœ‰çˆ±å¿ƒçš„äººä¼ é€’ä¸‹',\n",
    "    '@å°¾ç†ŠC è¦æå‰é¢„ä¹ è‚²å„¿çŸ¥è¯†çš„è¯ï¼Œå»ºè®®çœ‹ä¸€äº›å°å·«å†™çš„ä¹¦ï¼Œå˜»å˜»',\n",
    "]\n",
    "test_label=[0,0,1]\n",
    "for i in range(len(test_list)):\n",
    "    predict_rumor_GRU(test_list[i],test_label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
