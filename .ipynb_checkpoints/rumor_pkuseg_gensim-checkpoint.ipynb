{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "# 魔法命令，使用后画图不用show了\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re# 引入正则\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.解压词向量并加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1解压词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2# 用来解压文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./embeddings/sgns.weibo.bigram\", 'wb') as new_file, open(\"./embeddings/sgns.weibo.bigram.bz2\", 'rb') as file:\n",
    "    decompressor = bz2.BZ2Decompressor()\n",
    "    for data in iter(lambda : file.read(100 * 1024), b''):\n",
    "        new_file.write(decompressor.decompress(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2加载词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors# gensim用来加载预训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_model = KeyedVectors.load_word2vec_format('./embeddings/sgns.weibo.bigram', \n",
    "                                             binary=False,\n",
    "                                             unicode_errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.语料预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1读取原始文本\n",
    "* weibo：DataFrame存储的博文及其对应标签\n",
    "* content：list存储的原始文本字符串\n",
    "* label：标签，1为非谣言"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_not_rumor</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>【超市手推车插入式摆放宛若同性爱姿势，专家建议取缔[汗]】教育专家王建立近日指出，超市的手推...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>紧急通知，汕头市出事了！揭阳市出事了！潮州、汕尾、深圳、广州都相继出事了！大家一定要互相转告...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>分手后，不要回想甜蜜往事，因为会让自己更痛苦；不要怀疑TA的决定，因为TA已经决定了；不要尝...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>注意啦！【打针西瓜】入夏，西瓜成为首选的消暑食品，但黑心商贩却把针头对准了尚未成熟的西瓜。“...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2012年2月3日当地时间下午3点41分有个印度妇女生育了11个小孩！ 瞬间就被震惊了</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_not_rumor                                            content\n",
       "0             0  【超市手推车插入式摆放宛若同性爱姿势，专家建议取缔[汗]】教育专家王建立近日指出，超市的手推...\n",
       "1             0  紧急通知，汕头市出事了！揭阳市出事了！潮州、汕尾、深圳、广州都相继出事了！大家一定要互相转告...\n",
       "2             1  分手后，不要回想甜蜜往事，因为会让自己更痛苦；不要怀疑TA的决定，因为TA已经决定了；不要尝...\n",
       "3             0  注意啦！【打针西瓜】入夏，西瓜成为首选的消暑食品，但黑心商贩却把针头对准了尚未成熟的西瓜。“...\n",
       "4             0        2012年2月3日当地时间下午3点41分有个印度妇女生育了11个小孩！ 瞬间就被震惊了"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weibo = pd.read_csv('./data/all_data.txt',sep='\\t', names=['is_not_rumor','content'],encoding='utf-8')\n",
    "weibo = weibo.dropna()#删除缺失值\n",
    "weibo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3387, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weibo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将DataFrame中的Series转换为list\n",
    "content = weibo.content.values.tolist()\n",
    "label=weibo.is_not_rumor.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['注意啦！【打针西瓜】入夏，西瓜成为首选的消暑食品，但黑心商贩却把针头对准了尚未成熟的西瓜。“打针西瓜”所注射的禁用食品添加剂甜蜜素和胭脂红！打过针的西瓜瓜瓤呈红色，汁液也很“丰富”，但没有一点西瓜味。所用添加剂破坏 肝脏、肾脏的功能、影响儿童智力发育等毒性！', '2012年2月3日当地时间下午3点41分有个印度妇女生育了11个小孩！ 瞬间就被震惊了']\n"
     ]
    }
   ],
   "source": [
    "print (content[3:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2进行分词和tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/lancopku/PKUSeg-python\n",
    "\n",
    "对每一条微博文本text，\n",
    "1. 去掉每个样本的标点符号；\n",
    "2. 用pkuseg分词，得到存放分词结果的cut_list；\n",
    "3. 去掉cut_list中的停用词得到cut_list_clean；\n",
    "3. 将分词结果cut_list_clean索引化（使用北京师范大学中文信息处理研究所与中国人民大学 DBIIR 实验室的研究者开源的\"chinese-word-vectors\"），这样每一例评价的文本变成一段索引数字，对应着预训练词向量模型中的词。\n",
    "\n",
    "将每个text的结果存到train_tokens中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkuseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入停用词\n",
    "stopwords=pd.read_csv(\"./stopwords/stopwords.txt\",index_col=False,sep=\"\\t\",quoting=3,names=['stopword'], encoding='utf-8')\n",
    "stopwords = stopwords.stopword.values.tolist()#转为list形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = pkuseg.pkuseg(model_name='web')  # 程序会自动下载所对应的细领域模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = []\n",
    "for text in content:\n",
    "    # 去掉标点\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]+\", \"\",text)\n",
    "    # pkuseg分词\n",
    "    cut_list = seg.cut(text)\n",
    "\n",
    "    #去除停用词\n",
    "    cut_list_clean=[]\n",
    "    for word in cut_list:\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        cut_list_clean.append(word)\n",
    "    \n",
    "    #索引化\n",
    "    for i, word in enumerate(cut_list_clean): # enumerate()\n",
    "        try:\n",
    "            # 将词转换为索引index\n",
    "            cut_list_clean[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            # 如果词不在字典中，则输出0\n",
    "            cut_list_clean[i] = 0\n",
    "    train_tokens.append(cut_list_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3索引长度标准化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为每段评语的长度是不一样的，如果单纯取最长的一个评语，并把其他评填充成同样的长度，这样十分浪费计算资源，所以取一个折衷的长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获得所有tokens的长度\n",
    "num_tokens = [len(tokens) for tokens in train_tokens]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# 取tokens平均值并加上两个tokens的标准差，\n",
    "# 假设tokens长度的分布为正态分布，则max_tokens这个值可以涵盖95%左右的样本\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### padding（填充）和truncating（修剪）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们把文本转换为tokens（索引）之后，每一串索引的长度并不相等，所以为了方便模型的训练我们需要把索引的长度标准化，上面我们选择了max_tokens个可以涵盖95%训练样本的长度，接下来我们进行padding和truncating，我们一般采用'pre'的方法，这会在文本索引的前面填充0，因为根据一些研究资料中的实践，如果在文本索引后面填充0的话，会对模型造成一些不良影响。 \n",
    "\n",
    "进行padding和truncating， 输入的train_tokens是一个list\n",
    "返回的train_pad是一个numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pad = pad_sequences(train_tokens, maxlen=max_tokens,\n",
    "                            padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4准备Embedding Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 作为模型的输入，需要准备一个维度为 (𝑛𝑢𝑚𝑤𝑜𝑟𝑑𝑠,𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔𝑑𝑖𝑚) 的embedding矩阵，num words代表使用的词汇的数量。\n",
    "\n",
    "\n",
    "* 不进行词向量的训练，而是使用预训练的词向量——北京师范大学中文信息处理研究所与中国人民大学 DBIIR 实验室的研究者开源的\"chinese-word-vectors\"；https://github.com/Embedding/Chinese-Word-Vectors ；emdedding dimension在现在使用的预训练词向量模型中是300，每一个词汇都用一个长度为300的向量表示。\n",
    "\n",
    "\n",
    "* 注意只选择使用前50k个使用频率最高的词，在这个预训练词向量模型中，一共有260万词汇量，如果全部使用在分类问题上会很浪费计算资源，因为训练样本很小，如果有更多的训练样本时，在分类问题上可以考虑减少使用的词汇量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words = 50000\n",
    "embedding_dim=300\n",
    "# 初始化embedding_matrix，之后在keras上进行应用\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "# embedding_matrix为一个 [num_words，embedding_dim] 的矩阵\n",
    "# 维度为 50000 * 300\n",
    "for i in range(num_words):\n",
    "    embedding_matrix[i,:] = cn_model[cn_model.index2word[i]]#前50000个index对应的词的词向量\n",
    "embedding_matrix = embedding_matrix.astype('float32')\n",
    "# 检查index是否对应，\n",
    "# 输出300意义为长度为300的embedding向量一一对应\n",
    "np.sum(cn_model[cn_model.index2word[333]] == embedding_matrix[333] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超出五万个词向量的词用0代替\n",
    "train_pad[train_pad>=num_words ] = 0\n",
    "\n",
    "# 准备target向量，前2000样本为1，后2000为0\n",
    "train_target = np.array(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.训练语料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from utils import Attention,convolution\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1划分训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行训练和测试样本的分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 90%的样本用来训练，剩余10%用来测试\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pad,\n",
    "                                                    train_target,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2搭建网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "* Embedding：使用预训练词向量，参数不可训练\n",
    "* 双向LSTM，参数64\n",
    "* 双向LSTM，参数32\n",
    "* 全连接层，参数1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 58, 300)           15000000  \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 58, 128)          186880    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 64)               41216     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,232,321\n",
      "Trainable params: 232,321\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_tokens,\n",
    "                    trainable=False))\n",
    "model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(units=32, return_sequences=False)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "optimizer=tf.keras.optimizers.legacy.Adam(lr=1e-3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU\n",
    "* Embedding：使用预训练词向量，参数不可训练\n",
    "* 双向GRU，参数128\n",
    "* 全连接层，参数32\n",
    "* 全连接层，参数1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 58, 300)           15000000  \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 64)               64128     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 390       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,064,525\n",
      "Trainable params: 64,525\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_tokens,\n",
    "                    trainable=False))\n",
    "model1.add(Bidirectional(GRU(32)))\n",
    "model1.add(Dense(6, activation='relu'))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "optimizer=tf.keras.optimizers.legacy.Adam(lr=1e-3)\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN+LSTM+ATTENTION\n",
    "* Embedding：使用预训练词向量，参数不可训练\n",
    "* 一维卷积层，参数64\n",
    "* 双向LSTM，参数64\n",
    "* 注意力层\n",
    "* 双向LSTM，参数32\n",
    "* 全连接层，参数64\n",
    "* 全连接层，参数1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 58, 300)           15000000  \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 58, 32)            9632      \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 58, 128)          49664     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " attention (Attention)       (None, 58, 128)           186       \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 32)                20608     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,082,267\n",
      "Trainable params: 82,267\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_tokens,\n",
    "                    trainable=False))\n",
    "model2.add(Conv1D(32,1,activation='relu'))\n",
    "model2.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "model2.add(Attention(return_sequences=True))\n",
    "model2.add(LSTM(units=32, return_sequences=False))\n",
    "model2.add(Dense(64, activation='relu'))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "optimizer=tf.keras.optimizers.legacy.Adam(lr=1e-3)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextCNN\n",
    "* Embedding：使用预训练词向量，参数不可训练\n",
    "* 一维卷积层，参数64\n",
    "* 双向LSTM，参数64\n",
    "* 注意力层\n",
    "* 双向LSTM，参数32\n",
    "* 全连接层，参数64\n",
    "* 全连接层，参数1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 58, 300)           15000000  \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 58, 300, 1)        0         \n",
      "                                                                 \n",
      " model (Functional)          (None, 1, 1, 192)         230592    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 192)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                1930      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 10)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,232,533\n",
      "Trainable params: 232,533\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_tokens,\n",
    "                    trainable=False))\n",
    "model3.add(Reshape((58,300, 1)))\n",
    "model3.add(convolution())\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(10, activation='relu'))\n",
    "model3.add(Dropout(0.2))\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3模型配置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型保存（断点续训）、early stoping、学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立一个权重的存储点\n",
    "checkpoint_save_path=\"./checkpoint/rumor_LSTM.ckpt\"\n",
    "if os.path.exists(checkpoint_save_path+'.index'):\n",
    "    print('----------load the model----------')\n",
    "    model.load_weights(checkpoint_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存参数和模型\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_save_path, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义early stoping如果3个epoch内validation loss没有改善则停止训练\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "# 自动降低learning rate\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1, min_lr=1e-8, patience=0,\n",
    "                                       verbose=1)\n",
    "# 定义callback函数\n",
    "callbacks = [\n",
    "    earlystopping, \n",
    "#    checkpoint,\n",
    "    lr_reduction\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================model1==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立一个权重的存储点\n",
    "checkpoint_save_path1=\"./checkpoint/rumor_GRU.ckpt\"\n",
    "if os.path.exists(checkpoint_save_path1+'.index'):\n",
    "    print('----------load the model----------')\n",
    "    model1.load_weights(checkpoint_save_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存参数和模型\n",
    "checkpoint1 = ModelCheckpoint(filepath=checkpoint_save_path1, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================model2==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立一个权重的存储点\n",
    "checkpoint_save_path2=\"./checkpoint/rumor_CNN_LSTM.ckpt\"\n",
    "if os.path.exists(checkpoint_save_path2+'.index'):\n",
    "    print('----------load the model----------')\n",
    "    model2.load_weights(checkpoint_save_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存参数和模型\n",
    "checkpoint2 = ModelCheckpoint(filepath=checkpoint_save_path2, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================model3==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立一个权重的存储点\n",
    "checkpoint_save_path3=\"./checkpoint/rumor_TextCNN.ckpt\"\n",
    "if os.path.exists(checkpoint_save_path3+'.index'):\n",
    "    print('----------load the model----------')\n",
    "    model3.load_weights(checkpoint_save_path3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存参数和模型\n",
    "checkpoint3 = ModelCheckpoint(filepath=checkpoint_save_path3, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "22/22 [==============================] - 14s 237ms/step - loss: 0.6394 - accuracy: 0.6416 - val_loss: 0.5483 - val_accuracy: 0.7213 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 3s 131ms/step - loss: 0.4756 - accuracy: 0.7816 - val_loss: 0.4588 - val_accuracy: 0.7705 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 3s 129ms/step - loss: 0.3995 - accuracy: 0.8268 - val_loss: 0.4297 - val_accuracy: 0.8131 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 3s 134ms/step - loss: 0.3537 - accuracy: 0.8494 - val_loss: 0.4026 - val_accuracy: 0.8262 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - 3s 133ms/step - loss: 0.2820 - accuracy: 0.8859 - val_loss: 0.3797 - val_accuracy: 0.8426 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "22/22 [==============================] - 3s 130ms/step - loss: 0.2269 - accuracy: 0.9107 - val_loss: 0.3764 - val_accuracy: 0.8623 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9282\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "22/22 [==============================] - 3s 128ms/step - loss: 0.1925 - accuracy: 0.9282 - val_loss: 0.4138 - val_accuracy: 0.8492 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.1381 - accuracy: 0.9533\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "22/22 [==============================] - 3s 131ms/step - loss: 0.1381 - accuracy: 0.9533 - val_loss: 0.4080 - val_accuracy: 0.8623 - lr: 1.0000e-04\n",
      "Epoch 9/20\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.1238 - accuracy: 0.9635\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "22/22 [==============================] - 3s 136ms/step - loss: 0.1238 - accuracy: 0.9635 - val_loss: 0.4165 - val_accuracy: 0.8557 - lr: 1.0000e-05\n",
      "Epoch 10/20\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.1204 - accuracy: 0.9635\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "22/22 [==============================] - 3s 129ms/step - loss: 0.1204 - accuracy: 0.9635 - val_loss: 0.4176 - val_accuracy: 0.8525 - lr: 1.0000e-06\n",
      "Epoch 11/20\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.1202 - accuracy: 0.9635\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "22/22 [==============================] - 3s 130ms/step - loss: 0.1202 - accuracy: 0.9635 - val_loss: 0.4176 - val_accuracy: 0.8525 - lr: 1.0000e-07\n",
      "Epoch 11: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e0881a39c8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,validation_split=0.1,epochs=20,batch_size=128,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================model1==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "22/22 [==============================] - 6s 83ms/step - loss: 0.6881 - accuracy: 0.5443 - val_loss: 0.6738 - val_accuracy: 0.6066 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.6584 - accuracy: 0.6139 - val_loss: 0.6378 - val_accuracy: 0.6754 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.5898 - accuracy: 0.7058 - val_loss: 0.5469 - val_accuracy: 0.7410 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.4579 - accuracy: 0.8035 - val_loss: 0.4619 - val_accuracy: 0.7836 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.3782 - accuracy: 0.8385 - val_loss: 0.4613 - val_accuracy: 0.7967 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.3322 - accuracy: 0.8629 - val_loss: 0.4462 - val_accuracy: 0.8000 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.2897 - accuracy: 0.8837 - val_loss: 0.4050 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.2464 - accuracy: 0.9115\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.2465 - accuracy: 0.9114 - val_loss: 0.4065 - val_accuracy: 0.8328 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.2156 - accuracy: 0.9209\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 0.2156 - accuracy: 0.9209 - val_loss: 0.4055 - val_accuracy: 0.8361 - lr: 1.0000e-04\n",
      "Epoch 10/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.2122 - accuracy: 0.9252\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.2119 - accuracy: 0.9253 - val_loss: 0.4054 - val_accuracy: 0.8361 - lr: 1.0000e-05\n",
      "Epoch 11/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.2112 - accuracy: 0.9267\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.2116 - accuracy: 0.9260 - val_loss: 0.4054 - val_accuracy: 0.8361 - lr: 1.0000e-06\n",
      "Epoch 12/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.2117 - accuracy: 0.9260\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.2115 - accuracy: 0.9260 - val_loss: 0.4054 - val_accuracy: 0.8361 - lr: 1.0000e-07\n",
      "Epoch 12: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e0a39cd448>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(X_train, y_train,validation_split=0.1,epochs=20,batch_size=128,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "22/22 [==============================] - 9s 151ms/step - loss: 0.6889 - accuracy: 0.5443 - val_loss: 0.6718 - val_accuracy: 0.5639 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 2s 87ms/step - loss: 0.5836 - accuracy: 0.6898 - val_loss: 0.4825 - val_accuracy: 0.7803 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.4459 - accuracy: 0.8028 - val_loss: 0.4487 - val_accuracy: 0.8066 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3865 - accuracy: 0.8396 - val_loss: 0.4285 - val_accuracy: 0.8164 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3549 - accuracy: 0.8596\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "22/22 [==============================] - 2s 95ms/step - loss: 0.3549 - accuracy: 0.8596 - val_loss: 0.4319 - val_accuracy: 0.8098 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "22/22 [==============================] - 2s 93ms/step - loss: 0.3312 - accuracy: 0.8724 - val_loss: 0.4216 - val_accuracy: 0.8230 - lr: 1.0000e-04\n",
      "Epoch 7/20\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3162 - accuracy: 0.8771\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "22/22 [==============================] - 2s 91ms/step - loss: 0.3162 - accuracy: 0.8771 - val_loss: 0.4341 - val_accuracy: 0.8328 - lr: 1.0000e-04\n",
      "Epoch 8/20\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3107 - accuracy: 0.8775\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "22/22 [==============================] - 2s 91ms/step - loss: 0.3107 - accuracy: 0.8775 - val_loss: 0.4343 - val_accuracy: 0.8295 - lr: 1.0000e-05\n",
      "Epoch 9/20\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3098 - accuracy: 0.8782\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3098 - accuracy: 0.8782 - val_loss: 0.4343 - val_accuracy: 0.8262 - lr: 1.0000e-06\n",
      "Epoch 10/20\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3097 - accuracy: 0.8786\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "22/22 [==============================] - 2s 96ms/step - loss: 0.3097 - accuracy: 0.8786 - val_loss: 0.4343 - val_accuracy: 0.8262 - lr: 1.0000e-07\n",
      "Epoch 11/20\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3097 - accuracy: 0.8786\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 1e-08.\n",
      "22/22 [==============================] - 2s 93ms/step - loss: 0.3097 - accuracy: 0.8786 - val_loss: 0.4343 - val_accuracy: 0.8262 - lr: 1.0000e-08\n",
      "Epoch 11: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e114446c88>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train, y_train,validation_split=0.1,epochs=20,batch_size=128,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "22/22 [==============================] - 2s 67ms/step - loss: 0.5803 - accuracy: 0.6774 - val_loss: 0.4335 - val_accuracy: 0.8164 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 1s 55ms/step - loss: 0.4081 - accuracy: 0.8166 - val_loss: 0.3876 - val_accuracy: 0.8295 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 1s 59ms/step - loss: 0.3041 - accuracy: 0.8709 - val_loss: 0.3466 - val_accuracy: 0.8656 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2276 - accuracy: 0.9114 - val_loss: 0.3442 - val_accuracy: 0.8656 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.1714 - accuracy: 0.9398 - val_loss: 0.3163 - val_accuracy: 0.8590 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.1236 - accuracy: 0.9647\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.1238 - accuracy: 0.9635 - val_loss: 0.3255 - val_accuracy: 0.8754 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.0967 - accuracy: 0.9762\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.0969 - accuracy: 0.9763 - val_loss: 0.3285 - val_accuracy: 0.8754 - lr: 1.0000e-04\n",
      "Epoch 8/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.0934 - accuracy: 0.9721\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.0936 - accuracy: 0.9719 - val_loss: 0.3286 - val_accuracy: 0.8754 - lr: 1.0000e-05\n",
      "Epoch 9/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.0905 - accuracy: 0.9773\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0910 - accuracy: 0.9767 - val_loss: 0.3289 - val_accuracy: 0.8787 - lr: 1.0000e-06\n",
      "Epoch 10/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.0970 - accuracy: 0.9717\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 0.0969 - accuracy: 0.9719 - val_loss: 0.3289 - val_accuracy: 0.8787 - lr: 1.0000e-07\n",
      "Epoch 10: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22d67f97208>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(X_train, y_train,validation_split=0.1,epochs=20,batch_size=128,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5应用于测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 25ms/step - loss: 0.4066 - accuracy: 0.8525\n",
      "Accuracy:85.25%\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 10ms/step - loss: 0.3883 - accuracy: 0.8555\n",
      "Accuracy:85.55%\n"
     ]
    }
   ],
   "source": [
    "result = model1.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 14ms/step - loss: 0.4187 - accuracy: 0.8407\n",
      "Accuracy:84.07%\n"
     ]
    }
   ],
   "source": [
    "result = model2.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 6ms/step - loss: 0.3267 - accuracy: 0.8732\n",
      "Accuracy:87.32%\n"
     ]
    }
   ],
   "source": [
    "result = model3.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
