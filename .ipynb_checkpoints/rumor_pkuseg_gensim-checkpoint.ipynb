{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "# é­”æ³•å‘½ä»¤ï¼Œä½¿ç”¨åç”»å›¾ä¸ç”¨showäº†\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re# å¼•å…¥æ­£åˆ™\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.è§£å‹è¯å‘é‡å¹¶åŠ è½½"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1è§£å‹è¯å‘é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2# ç”¨æ¥è§£å‹æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./embeddings/sgns.weibo.bigram\", 'wb') as new_file, open(\"./embeddings/sgns.weibo.bigram.bz2\", 'rb') as file:\n",
    "    decompressor = bz2.BZ2Decompressor()\n",
    "    for data in iter(lambda : file.read(100 * 1024), b''):\n",
    "        new_file.write(decompressor.decompress(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2åŠ è½½è¯å‘é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors# gensimç”¨æ¥åŠ è½½é¢„è®­ç»ƒè¯å‘é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_model = KeyedVectors.load_word2vec_format('./embeddings/sgns.weibo.bigram', \n",
    "                                             binary=False,\n",
    "                                             unicode_errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.è¯­æ–™é¢„å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1è¯»å–åŸå§‹æ–‡æœ¬\n",
    "* weiboï¼šDataFrameå­˜å‚¨çš„åšæ–‡åŠå…¶å¯¹åº”æ ‡ç­¾\n",
    "* contentï¼šlistå­˜å‚¨çš„åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²\n",
    "* labelï¼šæ ‡ç­¾ï¼Œ1ä¸ºéè°£è¨€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_not_rumor</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ã€è¶…å¸‚æ‰‹æ¨è½¦æ’å…¥å¼æ‘†æ”¾å®›è‹¥åŒæ€§çˆ±å§¿åŠ¿ï¼Œä¸“å®¶å»ºè®®å–ç¼”[æ±—]ã€‘æ•™è‚²ä¸“å®¶ç‹å»ºç«‹è¿‘æ—¥æŒ‡å‡ºï¼Œè¶…å¸‚çš„æ‰‹æ¨...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ç´§æ€¥é€šçŸ¥ï¼Œæ±•å¤´å¸‚å‡ºäº‹äº†ï¼æ­é˜³å¸‚å‡ºäº‹äº†ï¼æ½®å·ã€æ±•å°¾ã€æ·±åœ³ã€å¹¿å·éƒ½ç›¸ç»§å‡ºäº‹äº†ï¼å¤§å®¶ä¸€å®šè¦äº’ç›¸è½¬å‘Š...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>åˆ†æ‰‹åï¼Œä¸è¦å›æƒ³ç”œèœœå¾€äº‹ï¼Œå› ä¸ºä¼šè®©è‡ªå·±æ›´ç—›è‹¦ï¼›ä¸è¦æ€€ç–‘TAçš„å†³å®šï¼Œå› ä¸ºTAå·²ç»å†³å®šäº†ï¼›ä¸è¦å°...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>æ³¨æ„å•¦ï¼ã€æ‰“é’ˆè¥¿ç“œã€‘å…¥å¤ï¼Œè¥¿ç“œæˆä¸ºé¦–é€‰çš„æ¶ˆæš‘é£Ÿå“ï¼Œä½†é»‘å¿ƒå•†è´©å´æŠŠé’ˆå¤´å¯¹å‡†äº†å°šæœªæˆç†Ÿçš„è¥¿ç“œã€‚â€œ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2012å¹´2æœˆ3æ—¥å½“åœ°æ—¶é—´ä¸‹åˆ3ç‚¹41åˆ†æœ‰ä¸ªå°åº¦å¦‡å¥³ç”Ÿè‚²äº†11ä¸ªå°å­©ï¼ ç¬é—´å°±è¢«éœ‡æƒŠäº†</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_not_rumor                                            content\n",
       "0             0  ã€è¶…å¸‚æ‰‹æ¨è½¦æ’å…¥å¼æ‘†æ”¾å®›è‹¥åŒæ€§çˆ±å§¿åŠ¿ï¼Œä¸“å®¶å»ºè®®å–ç¼”[æ±—]ã€‘æ•™è‚²ä¸“å®¶ç‹å»ºç«‹è¿‘æ—¥æŒ‡å‡ºï¼Œè¶…å¸‚çš„æ‰‹æ¨...\n",
       "1             0  ç´§æ€¥é€šçŸ¥ï¼Œæ±•å¤´å¸‚å‡ºäº‹äº†ï¼æ­é˜³å¸‚å‡ºäº‹äº†ï¼æ½®å·ã€æ±•å°¾ã€æ·±åœ³ã€å¹¿å·éƒ½ç›¸ç»§å‡ºäº‹äº†ï¼å¤§å®¶ä¸€å®šè¦äº’ç›¸è½¬å‘Š...\n",
       "2             1  åˆ†æ‰‹åï¼Œä¸è¦å›æƒ³ç”œèœœå¾€äº‹ï¼Œå› ä¸ºä¼šè®©è‡ªå·±æ›´ç—›è‹¦ï¼›ä¸è¦æ€€ç–‘TAçš„å†³å®šï¼Œå› ä¸ºTAå·²ç»å†³å®šäº†ï¼›ä¸è¦å°...\n",
       "3             0  æ³¨æ„å•¦ï¼ã€æ‰“é’ˆè¥¿ç“œã€‘å…¥å¤ï¼Œè¥¿ç“œæˆä¸ºé¦–é€‰çš„æ¶ˆæš‘é£Ÿå“ï¼Œä½†é»‘å¿ƒå•†è´©å´æŠŠé’ˆå¤´å¯¹å‡†äº†å°šæœªæˆç†Ÿçš„è¥¿ç“œã€‚â€œ...\n",
       "4             0        2012å¹´2æœˆ3æ—¥å½“åœ°æ—¶é—´ä¸‹åˆ3ç‚¹41åˆ†æœ‰ä¸ªå°åº¦å¦‡å¥³ç”Ÿè‚²äº†11ä¸ªå°å­©ï¼ ç¬é—´å°±è¢«éœ‡æƒŠäº†"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weibo = pd.read_csv('./data/all_data.txt',sep='\\t', names=['is_not_rumor','content'],encoding='utf-8')\n",
    "weibo = weibo.dropna()#åˆ é™¤ç¼ºå¤±å€¼\n",
    "weibo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3387, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weibo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å°†DataFrameä¸­çš„Seriesè½¬æ¢ä¸ºlist\n",
    "content = weibo.content.values.tolist()\n",
    "label=weibo.is_not_rumor.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['æ³¨æ„å•¦ï¼ã€æ‰“é’ˆè¥¿ç“œã€‘å…¥å¤ï¼Œè¥¿ç“œæˆä¸ºé¦–é€‰çš„æ¶ˆæš‘é£Ÿå“ï¼Œä½†é»‘å¿ƒå•†è´©å´æŠŠé’ˆå¤´å¯¹å‡†äº†å°šæœªæˆç†Ÿçš„è¥¿ç“œã€‚â€œæ‰“é’ˆè¥¿ç“œâ€æ‰€æ³¨å°„çš„ç¦ç”¨é£Ÿå“æ·»åŠ å‰‚ç”œèœœç´ å’Œèƒ­è„‚çº¢ï¼æ‰“è¿‡é’ˆçš„è¥¿ç“œç“œç“¤å‘ˆçº¢è‰²ï¼Œæ±æ¶²ä¹Ÿå¾ˆâ€œä¸°å¯Œâ€ï¼Œä½†æ²¡æœ‰ä¸€ç‚¹è¥¿ç“œå‘³ã€‚æ‰€ç”¨æ·»åŠ å‰‚ç ´å è‚è„ã€è‚¾è„çš„åŠŸèƒ½ã€å½±å“å„¿ç«¥æ™ºåŠ›å‘è‚²ç­‰æ¯’æ€§ï¼', '2012å¹´2æœˆ3æ—¥å½“åœ°æ—¶é—´ä¸‹åˆ3ç‚¹41åˆ†æœ‰ä¸ªå°åº¦å¦‡å¥³ç”Ÿè‚²äº†11ä¸ªå°å­©ï¼ ç¬é—´å°±è¢«éœ‡æƒŠäº†']\n"
     ]
    }
   ],
   "source": [
    "print (content[3:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2è¿›è¡Œåˆ†è¯å’Œtokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/lancopku/PKUSeg-python\n",
    "\n",
    "å¯¹æ¯ä¸€æ¡å¾®åšæ–‡æœ¬textï¼Œ\n",
    "1. å»æ‰æ¯ä¸ªæ ·æœ¬çš„æ ‡ç‚¹ç¬¦å·ï¼›\n",
    "2. ç”¨pkusegåˆ†è¯ï¼Œå¾—åˆ°å­˜æ”¾åˆ†è¯ç»“æœçš„cut_listï¼›\n",
    "3. å»æ‰cut_listä¸­çš„åœç”¨è¯å¾—åˆ°cut_list_cleanï¼›\n",
    "3. å°†åˆ†è¯ç»“æœcut_list_cleanç´¢å¼•åŒ–ï¼ˆä½¿ç”¨åŒ—äº¬å¸ˆèŒƒå¤§å­¦ä¸­æ–‡ä¿¡æ¯å¤„ç†ç ”ç©¶æ‰€ä¸ä¸­å›½äººæ°‘å¤§å­¦ DBIIR å®éªŒå®¤çš„ç ”ç©¶è€…å¼€æºçš„\"chinese-word-vectors\"ï¼‰ï¼Œè¿™æ ·æ¯ä¸€ä¾‹è¯„ä»·çš„æ–‡æœ¬å˜æˆä¸€æ®µç´¢å¼•æ•°å­—ï¼Œå¯¹åº”ç€é¢„è®­ç»ƒè¯å‘é‡æ¨¡å‹ä¸­çš„è¯ã€‚\n",
    "\n",
    "å°†æ¯ä¸ªtextçš„ç»“æœå­˜åˆ°train_tokensä¸­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkuseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å¯¼å…¥åœç”¨è¯\n",
    "stopwords=pd.read_csv(\"./stopwords/stopwords.txt\",index_col=False,sep=\"\\t\",quoting=3,names=['stopword'], encoding='utf-8')\n",
    "stopwords = stopwords.stopword.values.tolist()#è½¬ä¸ºlistå½¢å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = pkuseg.pkuseg(model_name='web')  # ç¨‹åºä¼šè‡ªåŠ¨ä¸‹è½½æ‰€å¯¹åº”çš„ç»†é¢†åŸŸæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = []\n",
    "for text in content:\n",
    "    # å»æ‰æ ‡ç‚¹\n",
    "    text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+â€”â€”ï¼ï¼Œã€‚ï¼Ÿã€~@#ï¿¥%â€¦â€¦&*ï¼ˆï¼‰]+\", \"\",text)\n",
    "    # pkusegåˆ†è¯\n",
    "    cut_list = seg.cut(text)\n",
    "\n",
    "    #å»é™¤åœç”¨è¯\n",
    "    cut_list_clean=[]\n",
    "    for word in cut_list:\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        cut_list_clean.append(word)\n",
    "    \n",
    "    #ç´¢å¼•åŒ–\n",
    "    for i, word in enumerate(cut_list_clean): # enumerate()\n",
    "        try:\n",
    "            # å°†è¯è½¬æ¢ä¸ºç´¢å¼•index\n",
    "            cut_list_clean[i] = cn_model.vocab[word].index\n",
    "        except KeyError:\n",
    "            # å¦‚æœè¯ä¸åœ¨å­—å…¸ä¸­ï¼Œåˆ™è¾“å‡º0\n",
    "            cut_list_clean[i] = 0\n",
    "    train_tokens.append(cut_list_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3ç´¢å¼•é•¿åº¦æ ‡å‡†åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å› ä¸ºæ¯æ®µè¯„è¯­çš„é•¿åº¦æ˜¯ä¸ä¸€æ ·çš„ï¼Œå¦‚æœå•çº¯å–æœ€é•¿çš„ä¸€ä¸ªè¯„è¯­ï¼Œå¹¶æŠŠå…¶ä»–è¯„å¡«å……æˆåŒæ ·çš„é•¿åº¦ï¼Œè¿™æ ·ååˆ†æµªè´¹è®¡ç®—èµ„æºï¼Œæ‰€ä»¥å–ä¸€ä¸ªæŠ˜è¡·çš„é•¿åº¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è·å¾—æ‰€æœ‰tokensçš„é•¿åº¦\n",
    "num_tokens = [len(tokens) for tokens in train_tokens]\n",
    "num_tokens = np.array(num_tokens)\n",
    "# å–tokenså¹³å‡å€¼å¹¶åŠ ä¸Šä¸¤ä¸ªtokensçš„æ ‡å‡†å·®ï¼Œ\n",
    "# å‡è®¾tokensé•¿åº¦çš„åˆ†å¸ƒä¸ºæ­£æ€åˆ†å¸ƒï¼Œåˆ™max_tokensè¿™ä¸ªå€¼å¯ä»¥æ¶µç›–95%å·¦å³çš„æ ·æœ¬\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### paddingï¼ˆå¡«å……ï¼‰å’Œtruncatingï¼ˆä¿®å‰ªï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬æŠŠæ–‡æœ¬è½¬æ¢ä¸ºtokensï¼ˆç´¢å¼•ï¼‰ä¹‹åï¼Œæ¯ä¸€ä¸²ç´¢å¼•çš„é•¿åº¦å¹¶ä¸ç›¸ç­‰ï¼Œæ‰€ä»¥ä¸ºäº†æ–¹ä¾¿æ¨¡å‹çš„è®­ç»ƒæˆ‘ä»¬éœ€è¦æŠŠç´¢å¼•çš„é•¿åº¦æ ‡å‡†åŒ–ï¼Œä¸Šé¢æˆ‘ä»¬é€‰æ‹©äº†max_tokensä¸ªå¯ä»¥æ¶µç›–95%è®­ç»ƒæ ·æœ¬çš„é•¿åº¦ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬è¿›è¡Œpaddingå’Œtruncatingï¼Œæˆ‘ä»¬ä¸€èˆ¬é‡‡ç”¨'pre'çš„æ–¹æ³•ï¼Œè¿™ä¼šåœ¨æ–‡æœ¬ç´¢å¼•çš„å‰é¢å¡«å……0ï¼Œå› ä¸ºæ ¹æ®ä¸€äº›ç ”ç©¶èµ„æ–™ä¸­çš„å®è·µï¼Œå¦‚æœåœ¨æ–‡æœ¬ç´¢å¼•åé¢å¡«å……0çš„è¯ï¼Œä¼šå¯¹æ¨¡å‹é€ æˆä¸€äº›ä¸è‰¯å½±å“ã€‚ \n",
    "\n",
    "è¿›è¡Œpaddingå’Œtruncatingï¼Œ è¾“å…¥çš„train_tokensæ˜¯ä¸€ä¸ªlist\n",
    "è¿”å›çš„train_padæ˜¯ä¸€ä¸ªnumpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pad = pad_sequences(train_tokens, maxlen=max_tokens,\n",
    "                            padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4å‡†å¤‡Embedding Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ä½œä¸ºæ¨¡å‹çš„è¾“å…¥ï¼Œéœ€è¦å‡†å¤‡ä¸€ä¸ªç»´åº¦ä¸º (ğ‘›ğ‘¢ğ‘šğ‘¤ğ‘œğ‘Ÿğ‘‘ğ‘ ,ğ‘’ğ‘šğ‘ğ‘’ğ‘‘ğ‘‘ğ‘–ğ‘›ğ‘”ğ‘‘ğ‘–ğ‘š) çš„embeddingçŸ©é˜µï¼Œnum wordsä»£è¡¨ä½¿ç”¨çš„è¯æ±‡çš„æ•°é‡ã€‚\n",
    "\n",
    "\n",
    "* ä¸è¿›è¡Œè¯å‘é‡çš„è®­ç»ƒï¼Œè€Œæ˜¯ä½¿ç”¨é¢„è®­ç»ƒçš„è¯å‘é‡â€”â€”åŒ—äº¬å¸ˆèŒƒå¤§å­¦ä¸­æ–‡ä¿¡æ¯å¤„ç†ç ”ç©¶æ‰€ä¸ä¸­å›½äººæ°‘å¤§å­¦ DBIIR å®éªŒå®¤çš„ç ”ç©¶è€…å¼€æºçš„\"chinese-word-vectors\"ï¼›https://github.com/Embedding/Chinese-Word-Vectors ï¼›emdedding dimensionåœ¨ç°åœ¨ä½¿ç”¨çš„é¢„è®­ç»ƒè¯å‘é‡æ¨¡å‹ä¸­æ˜¯300ï¼Œæ¯ä¸€ä¸ªè¯æ±‡éƒ½ç”¨ä¸€ä¸ªé•¿åº¦ä¸º300çš„å‘é‡è¡¨ç¤ºã€‚\n",
    "\n",
    "\n",
    "* æ³¨æ„åªé€‰æ‹©ä½¿ç”¨å‰50kä¸ªä½¿ç”¨é¢‘ç‡æœ€é«˜çš„è¯ï¼Œåœ¨è¿™ä¸ªé¢„è®­ç»ƒè¯å‘é‡æ¨¡å‹ä¸­ï¼Œä¸€å…±æœ‰260ä¸‡è¯æ±‡é‡ï¼Œå¦‚æœå…¨éƒ¨ä½¿ç”¨åœ¨åˆ†ç±»é—®é¢˜ä¸Šä¼šå¾ˆæµªè´¹è®¡ç®—èµ„æºï¼Œå› ä¸ºè®­ç»ƒæ ·æœ¬å¾ˆå°ï¼Œå¦‚æœæœ‰æ›´å¤šçš„è®­ç»ƒæ ·æœ¬æ—¶ï¼Œåœ¨åˆ†ç±»é—®é¢˜ä¸Šå¯ä»¥è€ƒè™‘å‡å°‘ä½¿ç”¨çš„è¯æ±‡é‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words = 50000\n",
    "embedding_dim=300\n",
    "# åˆå§‹åŒ–embedding_matrixï¼Œä¹‹ååœ¨kerasä¸Šè¿›è¡Œåº”ç”¨\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "# embedding_matrixä¸ºä¸€ä¸ª [num_wordsï¼Œembedding_dim] çš„çŸ©é˜µ\n",
    "# ç»´åº¦ä¸º 50000 * 300\n",
    "for i in range(num_words):\n",
    "    embedding_matrix[i,:] = cn_model[cn_model.index2word[i]]#å‰50000ä¸ªindexå¯¹åº”çš„è¯çš„è¯å‘é‡\n",
    "embedding_matrix = embedding_matrix.astype('float32')\n",
    "# æ£€æŸ¥indexæ˜¯å¦å¯¹åº”ï¼Œ\n",
    "# è¾“å‡º300æ„ä¹‰ä¸ºé•¿åº¦ä¸º300çš„embeddingå‘é‡ä¸€ä¸€å¯¹åº”\n",
    "np.sum(cn_model[cn_model.index2word[333]] == embedding_matrix[333] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¶…å‡ºäº”ä¸‡ä¸ªè¯å‘é‡çš„è¯ç”¨0ä»£æ›¿\n",
    "train_pad[train_pad>=num_words ] = 0\n",
    "\n",
    "# å‡†å¤‡targetå‘é‡ï¼Œå‰2000æ ·æœ¬ä¸º1ï¼Œå2000ä¸º0\n",
    "train_target = np.array(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.è®­ç»ƒè¯­æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from utils import Attention,convolution\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•æ ·æœ¬çš„åˆ†å‰²\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 90%çš„æ ·æœ¬ç”¨æ¥è®­ç»ƒï¼Œå‰©ä½™10%ç”¨æ¥æµ‹è¯•\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pad,\n",
    "                                                    train_target,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2æ­å»ºç½‘ç»œç»“æ„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "* Embeddingï¼šä½¿ç”¨é¢„è®­ç»ƒè¯å‘é‡ï¼Œå‚æ•°ä¸å¯è®­ç»ƒ\n",
    "* åŒå‘LSTMï¼Œå‚æ•°64\n",
    "* åŒå‘LSTMï¼Œå‚æ•°32\n",
    "* å…¨è¿æ¥å±‚ï¼Œå‚æ•°1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 58, 300)           15000000  \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 58, 128)          186880    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 64)               41216     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,232,321\n",
      "Trainable params: 232,321\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_tokens,\n",
    "                    trainable=False))\n",
    "model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(units=32, return_sequences=False)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "optimizer=tf.keras.optimizers.legacy.Adam(lr=1e-3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU\n",
    "* Embeddingï¼šä½¿ç”¨é¢„è®­ç»ƒè¯å‘é‡ï¼Œå‚æ•°ä¸å¯è®­ç»ƒ\n",
    "* åŒå‘GRUï¼Œå‚æ•°128\n",
    "* å…¨è¿æ¥å±‚ï¼Œå‚æ•°32\n",
    "* å…¨è¿æ¥å±‚ï¼Œå‚æ•°1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 58, 300)           15000000  \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 64)               64128     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 390       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,064,525\n",
      "Trainable params: 64,525\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_tokens,\n",
    "                    trainable=False))\n",
    "model1.add(Bidirectional(GRU(32)))\n",
    "model1.add(Dense(6, activation='relu'))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "optimizer=tf.keras.optimizers.legacy.Adam(lr=1e-3)\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN+LSTM+ATTENTION\n",
    "* Embeddingï¼šä½¿ç”¨é¢„è®­ç»ƒè¯å‘é‡ï¼Œå‚æ•°ä¸å¯è®­ç»ƒ\n",
    "* ä¸€ç»´å·ç§¯å±‚ï¼Œå‚æ•°64\n",
    "* åŒå‘LSTMï¼Œå‚æ•°64\n",
    "* æ³¨æ„åŠ›å±‚\n",
    "* åŒå‘LSTMï¼Œå‚æ•°32\n",
    "* å…¨è¿æ¥å±‚ï¼Œå‚æ•°64\n",
    "* å…¨è¿æ¥å±‚ï¼Œå‚æ•°1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 58, 300)           15000000  \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 58, 32)            9632      \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 58, 128)          49664     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " attention (Attention)       (None, 58, 128)           186       \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 32)                20608     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,082,267\n",
      "Trainable params: 82,267\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_tokens,\n",
    "                    trainable=False))\n",
    "model2.add(Conv1D(32,1,activation='relu'))\n",
    "model2.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "model2.add(Attention(return_sequences=True))\n",
    "model2.add(LSTM(units=32, return_sequences=False))\n",
    "model2.add(Dense(64, activation='relu'))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "optimizer=tf.keras.optimizers.legacy.Adam(lr=1e-3)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextCNN\n",
    "* Embeddingï¼šä½¿ç”¨é¢„è®­ç»ƒè¯å‘é‡ï¼Œå‚æ•°ä¸å¯è®­ç»ƒ\n",
    "* ä¸€ç»´å·ç§¯å±‚ï¼Œå‚æ•°64\n",
    "* åŒå‘LSTMï¼Œå‚æ•°64\n",
    "* æ³¨æ„åŠ›å±‚\n",
    "* åŒå‘LSTMï¼Œå‚æ•°32\n",
    "* å…¨è¿æ¥å±‚ï¼Œå‚æ•°64\n",
    "* å…¨è¿æ¥å±‚ï¼Œå‚æ•°1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 58, 300)           15000000  \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 58, 300, 1)        0         \n",
      "                                                                 \n",
      " model (Functional)          (None, 1, 1, 192)         230592    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 192)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                1930      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 10)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,232,533\n",
      "Trainable params: 232,533\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Embedding(num_words,\n",
    "                    embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_tokens,\n",
    "                    trainable=False))\n",
    "model3.add(Reshape((58,300, 1)))\n",
    "model3.add(convolution())\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(10, activation='relu'))\n",
    "model3.add(Dropout(0.2))\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3æ¨¡å‹é…ç½®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ¨¡å‹ä¿å­˜ï¼ˆæ–­ç‚¹ç»­è®­ï¼‰ã€early stopingã€å­¦ä¹ ç‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹ä¸€ä¸ªæƒé‡çš„å­˜å‚¨ç‚¹\n",
    "checkpoint_save_path=\"./checkpoint/rumor_LSTM.ckpt\"\n",
    "if os.path.exists(checkpoint_save_path+'.index'):\n",
    "    print('----------load the model----------')\n",
    "    model.load_weights(checkpoint_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ä¿å­˜å‚æ•°å’Œæ¨¡å‹\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_save_path, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰early stopingå¦‚æœ3ä¸ªepochå†…validation lossæ²¡æœ‰æ”¹å–„åˆ™åœæ­¢è®­ç»ƒ\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "# è‡ªåŠ¨é™ä½learning rate\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1, min_lr=1e-8, patience=0,\n",
    "                                       verbose=1)\n",
    "# å®šä¹‰callbackå‡½æ•°\n",
    "callbacks = [\n",
    "    earlystopping, \n",
    "#    checkpoint,\n",
    "    lr_reduction\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================model1==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹ä¸€ä¸ªæƒé‡çš„å­˜å‚¨ç‚¹\n",
    "checkpoint_save_path1=\"./checkpoint/rumor_GRU.ckpt\"\n",
    "if os.path.exists(checkpoint_save_path1+'.index'):\n",
    "    print('----------load the model----------')\n",
    "    model1.load_weights(checkpoint_save_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ä¿å­˜å‚æ•°å’Œæ¨¡å‹\n",
    "checkpoint1 = ModelCheckpoint(filepath=checkpoint_save_path1, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================model2==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹ä¸€ä¸ªæƒé‡çš„å­˜å‚¨ç‚¹\n",
    "checkpoint_save_path2=\"./checkpoint/rumor_CNN_LSTM.ckpt\"\n",
    "if os.path.exists(checkpoint_save_path2+'.index'):\n",
    "    print('----------load the model----------')\n",
    "    model2.load_weights(checkpoint_save_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ä¿å­˜å‚æ•°å’Œæ¨¡å‹\n",
    "checkpoint2 = ModelCheckpoint(filepath=checkpoint_save_path2, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================model3==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹ä¸€ä¸ªæƒé‡çš„å­˜å‚¨ç‚¹\n",
    "checkpoint_save_path3=\"./checkpoint/rumor_TextCNN.ckpt\"\n",
    "if os.path.exists(checkpoint_save_path3+'.index'):\n",
    "    print('----------load the model----------')\n",
    "    model3.load_weights(checkpoint_save_path3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ä¿å­˜å‚æ•°å’Œæ¨¡å‹\n",
    "checkpoint3 = ModelCheckpoint(filepath=checkpoint_save_path3, monitor='val_loss',\n",
    "                                      verbose=1, save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4è®­ç»ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "22/22 [==============================] - 14s 237ms/step - loss: 0.6394 - accuracy: 0.6416 - val_loss: 0.5483 - val_accuracy: 0.7213 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 3s 131ms/step - loss: 0.4756 - accuracy: 0.7816 - val_loss: 0.4588 - val_accuracy: 0.7705 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 3s 129ms/step - loss: 0.3995 - accuracy: 0.8268 - val_loss: 0.4297 - val_accuracy: 0.8131 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 3s 134ms/step - loss: 0.3537 - accuracy: 0.8494 - val_loss: 0.4026 - val_accuracy: 0.8262 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - 3s 133ms/step - loss: 0.2820 - accuracy: 0.8859 - val_loss: 0.3797 - val_accuracy: 0.8426 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "22/22 [==============================] - 3s 130ms/step - loss: 0.2269 - accuracy: 0.9107 - val_loss: 0.3764 - val_accuracy: 0.8623 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9282\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "22/22 [==============================] - 3s 128ms/step - loss: 0.1925 - accuracy: 0.9282 - val_loss: 0.4138 - val_accuracy: 0.8492 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.1381 - accuracy: 0.9533\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "22/22 [==============================] - 3s 131ms/step - loss: 0.1381 - accuracy: 0.9533 - val_loss: 0.4080 - val_accuracy: 0.8623 - lr: 1.0000e-04\n",
      "Epoch 9/20\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.1238 - accuracy: 0.9635\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "22/22 [==============================] - 3s 136ms/step - loss: 0.1238 - accuracy: 0.9635 - val_loss: 0.4165 - val_accuracy: 0.8557 - lr: 1.0000e-05\n",
      "Epoch 10/20\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.1204 - accuracy: 0.9635\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "22/22 [==============================] - 3s 129ms/step - loss: 0.1204 - accuracy: 0.9635 - val_loss: 0.4176 - val_accuracy: 0.8525 - lr: 1.0000e-06\n",
      "Epoch 11/20\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.1202 - accuracy: 0.9635\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "22/22 [==============================] - 3s 130ms/step - loss: 0.1202 - accuracy: 0.9635 - val_loss: 0.4176 - val_accuracy: 0.8525 - lr: 1.0000e-07\n",
      "Epoch 11: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e0881a39c8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,validation_split=0.1,epochs=20,batch_size=128,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================model1==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "22/22 [==============================] - 6s 83ms/step - loss: 0.6881 - accuracy: 0.5443 - val_loss: 0.6738 - val_accuracy: 0.6066 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.6584 - accuracy: 0.6139 - val_loss: 0.6378 - val_accuracy: 0.6754 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.5898 - accuracy: 0.7058 - val_loss: 0.5469 - val_accuracy: 0.7410 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.4579 - accuracy: 0.8035 - val_loss: 0.4619 - val_accuracy: 0.7836 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.3782 - accuracy: 0.8385 - val_loss: 0.4613 - val_accuracy: 0.7967 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.3322 - accuracy: 0.8629 - val_loss: 0.4462 - val_accuracy: 0.8000 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "22/22 [==============================] - 1s 38ms/step - loss: 0.2897 - accuracy: 0.8837 - val_loss: 0.4050 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.2464 - accuracy: 0.9115\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.2465 - accuracy: 0.9114 - val_loss: 0.4065 - val_accuracy: 0.8328 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.2156 - accuracy: 0.9209\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "22/22 [==============================] - 1s 43ms/step - loss: 0.2156 - accuracy: 0.9209 - val_loss: 0.4055 - val_accuracy: 0.8361 - lr: 1.0000e-04\n",
      "Epoch 10/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.2122 - accuracy: 0.9252\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.2119 - accuracy: 0.9253 - val_loss: 0.4054 - val_accuracy: 0.8361 - lr: 1.0000e-05\n",
      "Epoch 11/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.2112 - accuracy: 0.9267\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "22/22 [==============================] - 1s 40ms/step - loss: 0.2116 - accuracy: 0.9260 - val_loss: 0.4054 - val_accuracy: 0.8361 - lr: 1.0000e-06\n",
      "Epoch 12/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.2117 - accuracy: 0.9260\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "22/22 [==============================] - 1s 39ms/step - loss: 0.2115 - accuracy: 0.9260 - val_loss: 0.4054 - val_accuracy: 0.8361 - lr: 1.0000e-07\n",
      "Epoch 12: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e0a39cd448>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(X_train, y_train,validation_split=0.1,epochs=20,batch_size=128,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "22/22 [==============================] - 9s 151ms/step - loss: 0.6889 - accuracy: 0.5443 - val_loss: 0.6718 - val_accuracy: 0.5639 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 2s 87ms/step - loss: 0.5836 - accuracy: 0.6898 - val_loss: 0.4825 - val_accuracy: 0.7803 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.4459 - accuracy: 0.8028 - val_loss: 0.4487 - val_accuracy: 0.8066 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3865 - accuracy: 0.8396 - val_loss: 0.4285 - val_accuracy: 0.8164 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3549 - accuracy: 0.8596\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "22/22 [==============================] - 2s 95ms/step - loss: 0.3549 - accuracy: 0.8596 - val_loss: 0.4319 - val_accuracy: 0.8098 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "22/22 [==============================] - 2s 93ms/step - loss: 0.3312 - accuracy: 0.8724 - val_loss: 0.4216 - val_accuracy: 0.8230 - lr: 1.0000e-04\n",
      "Epoch 7/20\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3162 - accuracy: 0.8771\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "22/22 [==============================] - 2s 91ms/step - loss: 0.3162 - accuracy: 0.8771 - val_loss: 0.4341 - val_accuracy: 0.8328 - lr: 1.0000e-04\n",
      "Epoch 8/20\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3107 - accuracy: 0.8775\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "22/22 [==============================] - 2s 91ms/step - loss: 0.3107 - accuracy: 0.8775 - val_loss: 0.4343 - val_accuracy: 0.8295 - lr: 1.0000e-05\n",
      "Epoch 9/20\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3098 - accuracy: 0.8782\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "22/22 [==============================] - 2s 90ms/step - loss: 0.3098 - accuracy: 0.8782 - val_loss: 0.4343 - val_accuracy: 0.8262 - lr: 1.0000e-06\n",
      "Epoch 10/20\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3097 - accuracy: 0.8786\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "22/22 [==============================] - 2s 96ms/step - loss: 0.3097 - accuracy: 0.8786 - val_loss: 0.4343 - val_accuracy: 0.8262 - lr: 1.0000e-07\n",
      "Epoch 11/20\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.3097 - accuracy: 0.8786\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 1e-08.\n",
      "22/22 [==============================] - 2s 93ms/step - loss: 0.3097 - accuracy: 0.8786 - val_loss: 0.4343 - val_accuracy: 0.8262 - lr: 1.0000e-08\n",
      "Epoch 11: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e114446c88>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train, y_train,validation_split=0.1,epochs=20,batch_size=128,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "22/22 [==============================] - 2s 67ms/step - loss: 0.5803 - accuracy: 0.6774 - val_loss: 0.4335 - val_accuracy: 0.8164 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 1s 55ms/step - loss: 0.4081 - accuracy: 0.8166 - val_loss: 0.3876 - val_accuracy: 0.8295 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 1s 59ms/step - loss: 0.3041 - accuracy: 0.8709 - val_loss: 0.3466 - val_accuracy: 0.8656 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 1s 51ms/step - loss: 0.2276 - accuracy: 0.9114 - val_loss: 0.3442 - val_accuracy: 0.8656 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.1714 - accuracy: 0.9398 - val_loss: 0.3163 - val_accuracy: 0.8590 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.1236 - accuracy: 0.9647\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.1238 - accuracy: 0.9635 - val_loss: 0.3255 - val_accuracy: 0.8754 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.0967 - accuracy: 0.9762\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.0969 - accuracy: 0.9763 - val_loss: 0.3285 - val_accuracy: 0.8754 - lr: 1.0000e-04\n",
      "Epoch 8/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.0934 - accuracy: 0.9721\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "22/22 [==============================] - 1s 53ms/step - loss: 0.0936 - accuracy: 0.9719 - val_loss: 0.3286 - val_accuracy: 0.8754 - lr: 1.0000e-05\n",
      "Epoch 9/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.0905 - accuracy: 0.9773\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "22/22 [==============================] - 1s 52ms/step - loss: 0.0910 - accuracy: 0.9767 - val_loss: 0.3289 - val_accuracy: 0.8787 - lr: 1.0000e-06\n",
      "Epoch 10/20\n",
      "21/22 [===========================>..] - ETA: 0s - loss: 0.0970 - accuracy: 0.9717\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "22/22 [==============================] - 1s 50ms/step - loss: 0.0969 - accuracy: 0.9719 - val_loss: 0.3289 - val_accuracy: 0.8787 - lr: 1.0000e-07\n",
      "Epoch 10: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22d67f97208>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(X_train, y_train,validation_split=0.1,epochs=20,batch_size=128,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5åº”ç”¨äºæµ‹è¯•é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 25ms/step - loss: 0.4066 - accuracy: 0.8525\n",
      "Accuracy:85.25%\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 10ms/step - loss: 0.3883 - accuracy: 0.8555\n",
      "Accuracy:85.55%\n"
     ]
    }
   ],
   "source": [
    "result = model1.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 14ms/step - loss: 0.4187 - accuracy: 0.8407\n",
      "Accuracy:84.07%\n"
     ]
    }
   ],
   "source": [
    "result = model2.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 6ms/step - loss: 0.3267 - accuracy: 0.8732\n",
      "Accuracy:87.32%\n"
     ]
    }
   ],
   "source": [
    "result = model3.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
